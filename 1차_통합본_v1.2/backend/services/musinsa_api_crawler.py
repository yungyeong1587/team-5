"""
ë¬´ì‹ ì‚¬ ë¦¬ë·° í¬ë¡¤ë§ ì„œë¹„ìŠ¤ (í†µí•©)

ëª¨ë“  í¬ë¡¤ë§ ê´€ë ¨ ê¸°ëŠ¥ì„ í•˜ë‚˜ë¡œ í†µí•©:
- URL ìœ íš¨ì„± ê²€ì‚¬
- API í˜¸ì¶œ ë° íŒŒì‹±
- ë¦¬ë·° í•„í„°ë§
- DB ì €ì¥/ì¡°íšŒ
"""
from sqlalchemy.orm import Session
import re
import time
import json
import requests
from typing import List, Dict, Optional
import logging
import pandas as pd
from pathlib import Path

logger = logging.getLogger(__name__)


class MusinsaCrawler:
    """ë¬´ì‹ ì‚¬ ë¦¬ë·° í¬ë¡¤ëŸ¬ (All-in-One)"""
    
    # ë¬´ì‹ ì‚¬ API ì—”ë“œí¬ì¸íŠ¸
    API_BASE_URL = "https://goods.musinsa.com/api2/review/v1/view/list"
    
    def __init__(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'ko-KR,ko;q=0.9',
            'Origin': 'https://www.musinsa.com',
            'Referer': 'https://www.musinsa.com/',
        })
    
    # ===== URL ìœ íš¨ì„± ê²€ì‚¬ =====
    
    @staticmethod
    def validate_url(url: str) -> tuple[bool, str, str]:
        """
        ë¬´ì‹ ì‚¬ URL ìœ íš¨ì„± ê²€ì‚¬
        
        Args:
            url: ë¬´ì‹ ì‚¬ ìƒí’ˆ URL
        
        Returns:
            (ìœ íš¨ ì—¬ë¶€, ì—ëŸ¬ ë©”ì‹œì§€, ìƒí’ˆ ID)
        """
        if not url:
            return False, "URLì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.", None
        
        # ë¬´ì‹ ì‚¬ URL íŒ¨í„´ ì²´í¬
        pattern = r'https?://(?:www\.)?musinsa\.com/products/(\d+)'
        match = re.match(pattern, url)
        
        if not match:
            return False, "ì˜¬ë°”ë¥¸ ë¬´ì‹ ì‚¬ ìƒí’ˆ URLì´ ì•„ë‹™ë‹ˆë‹¤. (ì˜ˆ: https://www.musinsa.com/products/3242941)", None
        
        product_id = match.group(1)
        return True, "", product_id
    
    def extract_product_id(self, url: str) -> Optional[str]:
        """
        URLì—ì„œ ìƒí’ˆ ID ì¶”ì¶œ
        ì˜ˆ: https://www.musinsa.com/products/3242941 -> 3242941
        """
        match = re.search(r'/products/(\d+)', url)
        return match.group(1) if match else None
    
    # ===== API í˜¸ì¶œ =====
    
    def fetch_reviews_page(
        self, 
        goods_no: str, 
        page: int = 0, 
        page_size: int = 100,
        sort: str = "up_cnt_desc"
    ) -> Dict:
        """
        íŠ¹ì • í˜ì´ì§€ì˜ ë¦¬ë·° ê°€ì ¸ì˜¤ê¸°
        
        Args:
            goods_no: ìƒí’ˆ ë²ˆí˜¸
            page: í˜ì´ì§€ ë²ˆí˜¸ (0ë¶€í„°)
            page_size: í•œ ë²ˆì— ê°€ì ¸ì˜¬ ë¦¬ë·° ìˆ˜ (ìµœëŒ€ 100)
            sort: ì •ë ¬ ë°©ì‹
                - up_cnt_desc: ë„ì›€ë¨ ìˆœ
                - reg_dt_desc: ìµœì‹ ìˆœ
                - grade_desc: ë³„ì  ë†’ì€ìˆœ
                - grade_asc: ë³„ì  ë‚®ì€ìˆœ
        
        Returns:
            API ì‘ë‹µ JSON
        """
        params = {
            'page': page,
            'pageSize': page_size,
            'goodsNo': goods_no,
            'sort': sort,
            'selectedSimilarNo': goods_no,
            'myFilter': 'false',
            'hasPhoto': 'false',
            'isExperience': 'false'
        }
        
        try:
            response = self.session.get(
                self.API_BASE_URL,
                params=params,
                timeout=10
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"API ìš”ì²­ ì‹¤íŒ¨ (page={page}): {e}")
            raise
    
    # ===== ë¦¬ë·° íŒŒì‹± =====
    
    @staticmethod
    def parse_review(review_data: Dict) -> Dict:
        """
        API ì‘ë‹µì—ì„œ ë¦¬ë·° ë°ì´í„° íŒŒì‹±
        
        Args:
            review_data: API ì‘ë‹µì˜ ê°œë³„ ë¦¬ë·° ë°ì´í„°
        
        Returns:
            íŒŒì‹±ëœ ë¦¬ë·° ë”•ì…”ë„ˆë¦¬
        """
        return {
            'review_id': review_data.get('no'),
            'text': (review_data.get('content') or '').strip(),
            # gradeê°€ ë¬¸ìì—´ "5"ë¡œ ì˜¤ë”ë¼ë„ intë¡œ ë³€í™˜í•´ì„œ ì €ì¥
            'rating': int(review_data.get('grade')) if review_data.get('grade') is not None else None,
            'author': review_data.get('userProfileInfo', {}).get('userNickName'),
            'author_profile': review_data.get('userImageFile'),
            'date': review_data.get('createDate'),
            'helpful_count': review_data.get('likeCount', 0),
            'unhelpful_count': 0,  # ì‘ë‹µì— ë”°ë¡œ ì—†ìœ¼ë‹ˆ 0ìœ¼ë¡œ
            'product_info': review_data.get('goodsOption'),
            'has_photo': False,
            'photo_urls': [],
            # í”„ë¡œí•„ì— í‚¤/ëª¸ë¬´ê²Œê°€ ìˆì„ ê²½ìš°
            'body_shape': None,
            'height': None,
            'weight': None,
    }
    
    # ===== ë¦¬ë·° í•„í„°ë§ =====
    
    @staticmethod
    def clean_text(text: str) -> str:
        """ë¦¬ë·° í…ìŠ¤íŠ¸ ì •ì œ"""
        if not text:
            return ""
        
        # ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    @staticmethod
    def filter_meaningful_reviews(reviews: List[Dict], min_length: int = 10) -> List[Dict]:
        """
        ì˜ë¯¸ ìˆëŠ” ë¦¬ë·°ë§Œ í•„í„°ë§
        
        Args:
            reviews: ì›ë³¸ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸
            min_length: ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´
        
        Returns:
            í•„í„°ë§ëœ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸
        """
        filtered = []
        
        for review in reviews:
            text = review.get('text', '')
            
            # ìµœì†Œ ê¸¸ì´ ì²´í¬
            if len(text) < min_length:
                continue
            
            # ì˜ë¯¸ ì—†ëŠ” ë¦¬ë·° íŒ¨í„´ ì œê±°
            meaningless_patterns = [
                r'^[ã…‹ã…ã…‡\s]+$',  # 'ã…‹ã…‹ã…‹', 'ã…ã…ã…' ë“±
                r'^[!?.]+$',  # íŠ¹ìˆ˜ë¬¸ìë§Œ
            ]
            
            is_meaningless = any(re.match(pattern, text) for pattern in meaningless_patterns)
            
            if not is_meaningless:
                review['text'] = MusinsaCrawler.clean_text(text)
                filtered.append(review)
        
        return filtered
    
    # ===== í¬ë¡¤ë§ ë©”ì¸ ë¡œì§ =====
    
    def crawl_all_reviews(
        self, 
        goods_no: str, 
        max_reviews: Optional[int] = None,
        page_size: int = 100,
        sort: str = "up_cnt_desc"
    ) -> List[Dict]:
        """
        ëª¨ë“  ë¦¬ë·° ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§€ë„¤ì´ì…˜ ìë™)
        
        Args:
            goods_no: ìƒí’ˆ ë²ˆí˜¸
            max_reviews: ìµœëŒ€ í¬ë¡¤ë§ ë¦¬ë·° ìˆ˜ (Noneì´ë©´ ì „ì²´)
            page_size: í˜ì´ì§€ë‹¹ ë¦¬ë·° ìˆ˜
            sort: ì •ë ¬ ë°©ì‹
        
        Returns:
            ë¦¬ë·° ë¦¬ìŠ¤íŠ¸
        """
        
        all_reviews = []
        page = 0
        
        while True:
            try:
                # API í˜¸ì¶œ
                logger.debug(f"í˜ì´ì§€ {page} ìš”ì²­ ì¤‘...")
                response_data = self.fetch_reviews_page(
                    goods_no=goods_no,
                    page=page,
                    page_size=page_size,
                    sort=sort
                )
                
                # ë¦¬ë·° ë°ì´í„° ì¶”ì¶œ
                data = response_data.get('data', {}) or {}
                reviews = data.get('list', [])
                total_count = data.get('total') or 0
                
                if not reviews:
                    logger.info(f"ë” ì´ìƒ ë¦¬ë·° ì—†ìŒ (ì´ {len(all_reviews)}ê°œ ìˆ˜ì§‘)")
                    break
                
                # ë¦¬ë·° íŒŒì‹±
                for review_data in reviews:
                    parsed = self.parse_review(review_data)

                    # â­ ë³„ì  1~5ë§Œ í—ˆìš© (0ì  ì œì™¸)
                    if parsed['rating'] is None or parsed['rating'] < 1:
                        continue
                    if parsed['text']:  # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë¦¬ë·°ë§Œ
                        all_reviews.append(parsed)
                
                logger.info(f"í˜ì´ì§€ {page}: {len(reviews)}ê°œ (ëˆ„ì : {len(all_reviews)}/{total_count})")
                
                # ìµœëŒ€ ë¦¬ë·° ìˆ˜ ì²´í¬
                if max_reviews and len(all_reviews) >= max_reviews:
                    all_reviews = all_reviews[:max_reviews]
                    logger.info(f"ìµœëŒ€ ë¦¬ë·° ìˆ˜ ë„ë‹¬: {max_reviews}ê°œ")
                    break
                
                # ëª¨ë“  ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ
                if len(all_reviews) >= total_count:
                    logger.info(f"ëª¨ë“  ë¦¬ë·° ìˆ˜ì§‘ ì™„ë£Œ: {len(all_reviews)}ê°œ")
                    break
                
                # ë‹¤ìŒ í˜ì´ì§€
                page += 1
                time.sleep(0.3)  # API ë¶€í•˜ ë°©ì§€
                
            except Exception as e:
                logger.error(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                break
        
        return all_reviews
    
    def crawl_reviews(
        self, 
        product_url: str, 
        max_reviews: Optional[int] = None,
    ) -> Dict:
        """
        ë¬´ì‹ ì‚¬ ë¦¬ë·° í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜
        
        Args:
            product_url: ë¬´ì‹ ì‚¬ ìƒí’ˆ URL
            max_reviews: ìµœëŒ€ í¬ë¡¤ë§ ë¦¬ë·° ìˆ˜
        
        Returns:
            {
                'success': bool,
                'message': str,
                'product_id': str,
                'product_url': str,
                'reviews': List[Dict],
                'raw_count': int,
                'filtered_count': int
            }
        """
        result = {
            'success': False,
            'message': '',
            'product_id': None,
            'product_url': product_url,
            'reviews': [],
            'raw_count': 0,
            'filtered_count': 0
        }
        
        try:
            # 1. URL ìœ íš¨ì„± ê²€ì‚¬
            is_valid, error_msg, product_id = self.validate_url(product_url)
            if not is_valid:
                result['message'] = error_msg
                return result
            
            result['product_id'] = product_id
            logger.info(f"[MusinsaCrawler] ìƒí’ˆ {product_id} í¬ë¡¤ë§ ì‹œì‘")
            
            # 2. ëª¨ë“  ë¦¬ë·° ìˆ˜ì§‘
            raw_reviews = self.crawl_all_reviews(
                goods_no=product_id,
                max_reviews=max_reviews,
                page_size=100
            )

            logger.info(f"[MusinsaCrawler] ìƒí’ˆ {product_id} í¬ë¡¤ë§ ê²°ê³¼: raw_reviews={len(raw_reviews)}")
            # 3. ë¦¬ë·° í•„í„°ë§
            filtered_reviews = raw_reviews
            # filtered_reviews = self.filter_meaningful_reviews(raw_reviews, min_length=10)
            
            logger.info(f"[MusinsaCrawler] ì™„ë£Œ: raw={len(raw_reviews)}, filtered={len(filtered_reviews)}")
            
            # 4. ê²°ê³¼ ë°˜í™˜
            result.update({
                'success': True,
                'message': f'{len(filtered_reviews)}ê°œì˜ ë¦¬ë·°ë¥¼ í¬ë¡¤ë§í–ˆìŠµë‹ˆë‹¤.',
                'reviews': filtered_reviews,
                'raw_count': len(raw_reviews),
                'filtered_count': len(filtered_reviews)
            })
            
        except Exception as e:
            logger.error(f"[MusinsaCrawler] ì˜¤ë¥˜: {e}")
            result['message'] = f'í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}'
        
        return result

# ===== í¸ì˜ í•¨ìˆ˜ (backward compatibility) =====

def crawl_musinsa_reviews(
    product_url: str,
    max_reviews: int = 1000,
) -> Dict:
    """
    ë¬´ì‹ ì‚¬ ë¦¬ë·° í¬ë¡¤ë§ í¸ì˜ í•¨ìˆ˜
    
    ì‚¬ìš© ì˜ˆ:
        result = crawl_musinsa_reviews(
            "https://www.musinsa.com/products/3242941",
            max_reviews=1000
        )
    """
    crawler = MusinsaCrawler()
    return crawler.crawl_reviews(
        product_url=product_url,
        max_reviews=max_reviews,
    )


def crawl_and_save_csv(product_url: str, max_reviews: int = 1000):
    """
    â¤ ë¬´ì‹ ì‚¬ ë¦¬ë·°ë¥¼ í¬ë¡¤ë§í•œ ë’¤ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    â¤ retrain_rf_model.pyì—ì„œ ê·¸ëŒ€ë¡œ í•™ìŠµ ê°€ëŠ¥í•¨
    
    CSV ì €ì¥ í˜•ì‹:
        review_text,rating
        "ë§Œì¡±í•©ë‹ˆë‹¤",5
        "ë³„ë¡œì—ìš”",1
    """

    crawler = MusinsaCrawler()
    result = crawler.crawl_reviews(product_url, max_reviews=max_reviews)

    if not result["success"]:
        print("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨:", result["message"])
        return None

    reviews = result["reviews"]
    product_id = result["product_id"]

    if not reviews:
        print("âŒ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # CSV ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸ ë³€í™˜
    rows = []
    for r in reviews:
        text = (r.get("text") or "").strip()
        rating = r.get("rating")

        if not text or rating is None:
            continue

        rows.append({
            "review_text": text,
            "rating": rating
        })

    df = pd.DataFrame(rows)

    # ì €ì¥ í´ë” ìƒì„±
    save_dir = Path("../dataset")
    save_dir.mkdir(exist_ok=True)

    save_path = save_dir / f"reviews_{product_id}.csv"
    df.to_csv(save_path, index=False, encoding="utf-8-sig")

    print("\n====================================================")
    print(f"ğŸ“„ CSV ì €ì¥ ì™„ë£Œ: {save_path.resolve()}")
    print(f"ì´ ë¦¬ë·° ê°œìˆ˜: {len(df)}ê°œ")
    print("====================================================\n")

    return save_path