"""
AI ë¦¬ë·° ë¶„ì„ê¸° (ë…ë¦½ ì‹¤í–‰)
ì…ë ¥: JSON (stdin ë˜ëŠ” íŒŒì¼)
ì¶œë ¥: JSON (stdout)
"""
import re
import sys
import json
import argparse
from pathlib import Path
import logging
import io

# í•œê¸€ ì¸ì½”ë”© ì„¤ì •
sys.stderr.flush()
sys.stdin = io.TextIOWrapper(sys.stdin.buffer, encoding='utf-8')
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stderr)]
)
logger = logging.getLogger(__name__)

class ReviewAIAnalyzer:
    """ë¦¬ë·° AI ë¶„ì„ê¸° (KcELECTRA ê¸°ë°˜)"""
    
    def __init__(self, model_path="ai_models", retrain_model_path="scripts/ai_models_retrained"):
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
        current_file = Path(__file__).resolve()
        project_root = current_file.parent.parent.parent
        self.base_model_path = project_root / model_path
        self.retrain_root = project_root / retrain_model_path
        self.model_path = self.get_latest_model_path() # ìµœì‹  ëª¨ë¸ ì“¸ ë•Œ
        #self.model_path = self.base_model_path # ê¸°ì¡´ ëª¨ë¸ ì“¸ ë•Œ

        self.model = None
        self.tokenizer = None
        self.device = None
        
        logger.info(f"AI ë¶„ì„ê¸° ì´ˆê¸°í™”: {self.model_path}")

    def get_latest_model_path(self):
        """ì¬í•™ìŠµëœ ëª¨ë¸ ì¤‘ ê°€ì¥ ìµœì‹  ë²„ì „ì„ ìë™ ì„ íƒ"""
        try:
            if not self.retrain_root.exists():
                logger.warning("ì¬í•™ìŠµ ëª¨ë¸ í´ë” ì—†ìŒ â†’ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
                return self.base_model_path

            # ì¬í•™ìŠµ í´ë” ì•ˆì˜ model_YYYYMMDD_HHMMSS ê°™ì€ í´ë” ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°
            model_dirs = [
                d for d in self.retrain_root.iterdir()
                if d.is_dir() and re.match(r"model_\d{8}_\d{6}", d.name)
            ]

            if not model_dirs:
                logger.warning("ì¬í•™ìŠµ ëª¨ë¸ ì—†ìŒ â†’ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
                return self.base_model_path

            # ê°€ì¥ ìµœê·¼ ëª¨ë¸ ì„ íƒ
            latest = max(model_dirs, key=lambda d: d.stat().st_mtime)

            logger.info(f"ğŸ“Œ ìµœì‹  ì¬í•™ìŠµ ëª¨ë¸ ì„ íƒë¨: {latest}")
            return latest

        except Exception as e:
            logger.error(f"âŒ ìµœì‹  ëª¨ë¸ íƒìƒ‰ ì‹¤íŒ¨: {e}")
            return self.base_model_path

    def load_model(self):
        """AI ëª¨ë¸ ë¡œë“œ"""
        try:
            import torch
            from transformers import AutoModelForSequenceClassification, AutoTokenizer
            
            logger.info("AI ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            
            self.tokenizer = AutoTokenizer.from_pretrained(str(self.model_path), local_files_only=True)
            self.model = AutoModelForSequenceClassification.from_pretrained(str(self.model_path), local_files_only=True)
            self.model.to(self.device)
            self.model.eval()
            
            logger.info(f"âœ… AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ (Device: {self.device})")
            return True
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def preprocess_reviews(self, reviews):
        """ì „ì²˜ë¦¬: í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë¦¬ë·°ë§Œ ì¶”ì¶œ"""
        processed = []
        for review in reviews:
            # textë‚˜ content í‚¤ê°€ ì„ì—¬ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‘˜ ë‹¤ í™•ì¸
            text = review.get('text', '') or review.get('content', '')
            if not text: continue
            
            # ì›ë³¸ ë°ì´í„° ë³´ì¡´í•˜ë©° text í•„ë“œ í†µì¼
            item = review.copy()
            item['text'] = text.strip()
            processed.append(item)
            
        return processed
    
    def analyze_reviews(self, reviews):
        """ë¦¬ë·° ë¶„ì„ ìˆ˜í–‰ ë° ê°œë³„ ì ìˆ˜ ë§ˆí‚¹"""
        try:
            import torch
            import torch.nn.functional as F
            
            logger.info(f"AI ë¶„ì„ ì‹œì‘: {len(reviews)}ê°œ ë¦¬ë·°")
            
            texts = [r['text'] for r in reviews]
            batch_size = 32
            all_trust_scores = []
            
            # ë°°ì¹˜ ë‹¨ìœ„ ì¶”ë¡  (ì†ë„ ìµœì í™”)
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i+batch_size]
                
                inputs = self.tokenizer(
                    batch_texts,
                    return_tensors="pt",
                    truncation=True,
                    max_length=128,
                    padding=True
                ).to(self.device)
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    probs = F.softmax(outputs.logits, dim=-1)
                    # 1ë²ˆ í´ë˜ìŠ¤(ì‹ ë¢°/ê¸ì •) í™•ë¥  ì¶”ì¶œ
                    trust_probs = probs[:, 1].cpu().tolist()
                    all_trust_scores.extend(trust_probs)
            
            # ---------------------------------------------------------
            # [í•µì‹¬ ìˆ˜ì •] ê°œë³„ ë¦¬ë·°ì— ì ìˆ˜ ë° ë¼ë²¨/ìƒ‰ìƒ ë¶€ì°©
            # ---------------------------------------------------------
            enriched_reviews = []
            for i, review in enumerate(reviews):
                score = all_trust_scores[i] * 100  # 0~100ì  ë³€í™˜
                
                # ë¼ë²¨ë§ ë° ìƒ‰ìƒ ê²°ì • (ìš”ì²­í•˜ì‹  ê¸°ì¤€)
                if score >= 76:
                    label = "ë§¤ìš° ë„ì›€ë¨"
                    color = "status-green"
                elif score >= 36:
                    label = "ë¶€ë¶„ì ìœ¼ë¡œ ë„ì›€ë¨"
                    color = "status-orange"
                else:
                    label = "ë„ì›€ ì•ˆë¨"
                    color = "status-red"
                
                # ê¸°ì¡´ ë¦¬ë·° ë°ì´í„°ì— ìƒˆ í•„ë“œ ì¶”ê°€
                review['reliability_score'] = round(score, 1)
                review['analysis_label'] = label
                review['color_class'] = color
                
                enriched_reviews.append(review)
            
            # ì „ì²´ í†µê³„ ê³„ì‚° (í‰ê·  ì‹ ë¢°ë„)
            avg_trust = sum(all_trust_scores) / len(all_trust_scores) if all_trust_scores else 0
            
            # ì „ì²´ íŒì • (Verdict)
            if avg_trust > 0.7:
                verdict = 'safe'
                confidence = avg_trust * 100
            elif avg_trust >= 0.3:
                verdict = 'suspicious'
                confidence = 50 + (avg_trust - 0.3) * 125
            else:
                verdict = 'malicious'
                confidence = 90 + (1 - avg_trust / 0.3) * 10
            
            result = {
                'verdict': verdict,
                'confidence': round(confidence, 2),
                'enriched_reviews': enriched_reviews,  # ì ìˆ˜ê°€ í¬í•¨ëœ ì „ì²´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                'details': {
                    'avg_trust_score': round(avg_trust, 4),
                    'total_reviews': len(reviews)
                }
            }
            return result
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise
    
    def run(self, reviews):
        if self.model is None:
            if not self.load_model():
                return {'verdict': 'error', 'confidence': 0, 'error': 'ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨'}
        
        processed_reviews = self.preprocess_reviews(reviews)
        if not processed_reviews:
            return {'verdict': 'error', 'confidence': 0, 'error': 'ë¶„ì„í•  ë¦¬ë·° ì—†ìŒ'}
            
        return self.analyze_reviews(processed_reviews)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', type=str)
    parser.add_argument('--output', type=str)
    parser.add_argument('--model', type=str, default='ai_models')
    args = parser.parse_args()
    
    try:
        if args.input:
            with open(args.input, 'r', encoding='utf-8') as f: data = json.load(f)
        else:
            data = json.load(sys.stdin)
        
        reviews = data.get('reviews', [])
        
        analyzer = ReviewAIAnalyzer(model_path=args.model)
        result = analyzer.run(reviews)
        
        output_data = {'success': True, 'result': result}
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
        else:
            print(json.dumps(output_data, ensure_ascii=False))
            
    except Exception as e:
        error_data = {'success': False, 'error': str(e)}
        print(json.dumps(error_data, ensure_ascii=False))

if __name__ == "__main__":
    main()