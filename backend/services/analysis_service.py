"""
ë¶„ì„ ì„œë¹„ìŠ¤ (ë…ë¦½ì )
"""
from sqlalchemy.orm import Session
from models.analysis import Analysis
from services.musinsa_api_crawler import MusinsaCrawler
from services.gemini_summarizer import GeminiSummarizer
import logging
import random
import json
import subprocess
import sys
import os

logger = logging.getLogger(__name__)

class AnalysisService:
    
    @staticmethod
    def create_analysis(db: Session, review_url: str) -> Analysis:
        analysis = Analysis(review_url=review_url, status='queued')
        db.add(analysis)
        db.commit()
        db.refresh(analysis)
        return analysis
    
    @staticmethod
    def get_analysis(db: Session, analysis_id: int) -> Analysis:
        return db.query(Analysis).filter(Analysis.analysis_id == analysis_id).first()
    
    @staticmethod
    def list_analyses(db: Session, status: str = None, skip: int = 0, limit: int = 10) -> list:
        query = db.query(Analysis)
        if status: query = query.filter(Analysis.status == status)
        analyses = query.order_by(Analysis.created_at.desc()).offset(skip).limit(limit).all()
        return [analysis.to_dict() for analysis in analyses]
    
    @staticmethod
    def update_analysis_status(db: Session, analysis_id: int, status: str, **kwargs):
        analysis = db.query(Analysis).filter(Analysis.analysis_id == analysis_id).first()
        if analysis:
            analysis.status = status
            for key, value in kwargs.items():
                if hasattr(analysis, key) and value is not None:
                    setattr(analysis, key, value)
            db.commit()
            db.refresh(analysis)
        return analysis
    
    @staticmethod
    async def analyze_with_ai(reviews: list, analysis_id: int, db: Session) -> dict:
        """AI ì„œë²„(subprocess) ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜"""
        try:
            input_data = {'reviews': reviews, 'analysis_id': analysis_id}
            input_json = json.dumps(input_data, ensure_ascii=False)
            
            my_env = os.environ.copy()
            my_env["PYTHONIOENCODING"] = "utf-8"

            # ai_analyzer.py ì‹¤í–‰ (íƒ€ìž„ì•„ì›ƒ 5ë¶„)
            process = subprocess.run(
                [sys.executable, 'services/ai_analyzer.py'],
                input=input_json,
                capture_output=True,
                text=True,
                encoding='utf-8', 
                env=my_env,
                timeout=300
            )
            
            if process.returncode != 0:
                raise Exception(f"AI í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜: {process.stderr}")
            
            output_data = json.loads(process.stdout)
            if not output_data.get('success'):
                raise Exception(output_data.get('error', 'Unknown Error'))
            
            return output_data['result']
            
        except Exception as e:
            logger.error(f"[Analysis {analysis_id}] AI ë¶„ì„ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ë¡œì§ ì¤‘ë‹¨ ë°©ì§€ë¥¼ ìœ„í•´ ê¸°ë³¸ê°’ ë°˜í™˜
            return {'verdict': 'error', 'confidence': 0, 'enriched_reviews': reviews}

    @staticmethod
    async def process_analysis(analysis_id: int, review_url: str, db: Session):
        """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸"""
        try:
            logger.info(f"[Analysis {analysis_id}] ðŸš€ ë¶„ì„ ì‹œìž‘")
            AnalysisService.update_analysis_status(db, analysis_id, 'processing')
            
            # 1. í¬ë¡¤ë§
            crawler = MusinsaCrawler()
            crawl_result = crawler.crawl_reviews(product_url=review_url, max_reviews=500)
            if not crawl_result['success']:
                raise Exception(crawl_result['message'])
            
            raw_reviews = crawl_result['reviews']
            logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(raw_reviews)}ê°œ")

            # 2. AI ë¶„ì„ ì‹¤í–‰
            ai_result = await AnalysisService.analyze_with_ai(raw_reviews, analysis_id, db)
            
            # ì ìˆ˜ê°€ í¬í•¨ëœ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ì›ë³¸)
            enriched_reviews = ai_result.get('enriched_reviews', raw_reviews)

            # 3. ë°ì´í„° í¬ë§· í†µì¼ ë° ìƒìœ„/í•˜ìœ„ ì¶”ì¶œ
            def normalize(r):
                """í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ í¬ë§· ë³€í™˜"""
                return {
                    "content": r.get('text') or r.get('content', ''),
                    "rating": int(r.get('rating', 0)),
                    "date": r.get('date', ''),
                    "author": r.get('author', '***'),
                    # AI ë¶„ì„ ê²°ê³¼ í•„ë“œ (ì¤‘ìš”!)
                    "reliability_score": r.get('reliability_score', 0),
                    "analysis_label": r.get('analysis_label', 'ë¶„ì„ ëŒ€ê¸°'),
                    "color_class": r.get('color_class', 'status-gray')
                }

            # í‰ì  ê¸°ì¤€ ë¶„ë¦¬
            high_rated = [r for r in enriched_reviews if r.get('rating', 0) >= 4]
            low_rated = [r for r in enriched_reviews if r.get('rating', 0) <= 2]
            
            # ëžœë¤ ì„žê¸°
            random.shuffle(high_rated)
            random.shuffle(low_rated)
            
            # ìƒìœ„ 10ê°œ, í•˜ìœ„ 10ê°œ ì¶”ì¶œ
            top_reviews = [normalize(r) for r in high_rated[:10]]
            worst_reviews = [normalize(r) for r in low_rated[:10]]

            # 4. Gemini ìš”ì•½
            gemini_summary = ""
            try:
                summarizer = GeminiSummarizer()
                gemini_summary = summarizer.summarize_reviews(raw_reviews, max_reviews=50)
            except Exception as e:
                logger.error(f"Gemini ìš”ì•½ ì‹¤íŒ¨: {e}")

            # 5. DB ì €ìž¥
            AnalysisService.update_analysis_status(
                db, analysis_id, 'completed',
                verdict=ai_result.get('verdict'),
                confidence=ai_result.get('confidence'),
                review_count=len(raw_reviews),
                top_reviews=top_reviews,      
                worst_reviews=worst_reviews,
                summary=gemini_summary
            )
            logger.info(f"[Analysis {analysis_id}] âœ… ë¶„ì„ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
            AnalysisService.update_analysis_status(db, analysis_id, 'failed', error_message=str(e))